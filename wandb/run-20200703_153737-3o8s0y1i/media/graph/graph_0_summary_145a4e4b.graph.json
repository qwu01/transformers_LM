{"format": "torch", "nodes": [{"name": "reformer", "id": 1621993846856, "class_name": "ReformerModel(\n  (embeddings): ReformerEmbeddings(\n    (word_embeddings): Embedding(30, 256)\n    (position_embeddings): AxialPositionEmbeddings(\n      (weights): ParameterList(\n          (0): Parameter containing: [torch.cuda.FloatTensor of size 64x1x64 (GPU 0)]\n          (1): Parameter containing: [torch.cuda.FloatTensor of size 1x72x192 (GPU 0)]\n      )\n    )\n  )\n  (encoder): ReformerEncoder(\n    (layers): ModuleList(\n      (0): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=256, out_features=128, bias=False)\n            (key): Linear(in_features=256, out_features=128, bias=False)\n            (value): Linear(in_features=256, out_features=128, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=128, out_features=256, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=256, out_features=512, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=512, out_features=256, bias=True)\n          )\n        )\n      )\n      (1): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LSHSelfAttention(\n            (query_key): Linear(in_features=256, out_features=128, bias=False)\n            (value): Linear(in_features=256, out_features=128, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=128, out_features=256, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=256, out_features=512, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=512, out_features=256, bias=True)\n          )\n        )\n      )\n      (2): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=256, out_features=128, bias=False)\n            (key): Linear(in_features=256, out_features=128, bias=False)\n            (value): Linear(in_features=256, out_features=128, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=128, out_features=256, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=256, out_features=512, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=512, out_features=256, bias=True)\n          )\n        )\n      )\n      (3): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LSHSelfAttention(\n            (query_key): Linear(in_features=256, out_features=128, bias=False)\n            (value): Linear(in_features=256, out_features=128, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=128, out_features=256, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=256, out_features=512, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=512, out_features=256, bias=True)\n          )\n        )\n      )\n      (4): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=256, out_features=128, bias=False)\n            (key): Linear(in_features=256, out_features=128, bias=False)\n            (value): Linear(in_features=256, out_features=128, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=128, out_features=256, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=256, out_features=512, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=512, out_features=256, bias=True)\n          )\n        )\n      )\n      (5): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LSHSelfAttention(\n            (query_key): Linear(in_features=256, out_features=128, bias=False)\n            (value): Linear(in_features=256, out_features=128, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=128, out_features=256, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=256, out_features=512, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=512, out_features=256, bias=True)\n          )\n        )\n      )\n    )\n    (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n  )\n)", "parameters": [["embeddings.word_embeddings.weight", [30, 256]], ["embeddings.position_embeddings.weights.0", [64, 1, 64]], ["embeddings.position_embeddings.weights.1", [1, 72, 192]], ["encoder.layers.0.attention.layer_norm.weight", [256]], ["encoder.layers.0.attention.layer_norm.bias", [256]], ["encoder.layers.0.attention.self_attention.query.weight", [128, 256]], ["encoder.layers.0.attention.self_attention.key.weight", [128, 256]], ["encoder.layers.0.attention.self_attention.value.weight", [128, 256]], ["encoder.layers.0.attention.output.dense.weight", [256, 128]], ["encoder.layers.0.feed_forward.layer_norm.weight", [256]], ["encoder.layers.0.feed_forward.layer_norm.bias", [256]], ["encoder.layers.0.feed_forward.dense.dense.weight", [512, 256]], ["encoder.layers.0.feed_forward.dense.dense.bias", [512]], ["encoder.layers.0.feed_forward.output.dense.weight", [256, 512]], ["encoder.layers.0.feed_forward.output.dense.bias", [256]], ["encoder.layers.1.attention.layer_norm.weight", [256]], ["encoder.layers.1.attention.layer_norm.bias", [256]], ["encoder.layers.1.attention.self_attention.query_key.weight", [128, 256]], ["encoder.layers.1.attention.self_attention.value.weight", [128, 256]], ["encoder.layers.1.attention.output.dense.weight", [256, 128]], ["encoder.layers.1.feed_forward.layer_norm.weight", [256]], ["encoder.layers.1.feed_forward.layer_norm.bias", [256]], ["encoder.layers.1.feed_forward.dense.dense.weight", [512, 256]], ["encoder.layers.1.feed_forward.dense.dense.bias", [512]], ["encoder.layers.1.feed_forward.output.dense.weight", [256, 512]], ["encoder.layers.1.feed_forward.output.dense.bias", [256]], ["encoder.layers.2.attention.layer_norm.weight", [256]], ["encoder.layers.2.attention.layer_norm.bias", [256]], ["encoder.layers.2.attention.self_attention.query.weight", [128, 256]], ["encoder.layers.2.attention.self_attention.key.weight", [128, 256]], ["encoder.layers.2.attention.self_attention.value.weight", [128, 256]], ["encoder.layers.2.attention.output.dense.weight", [256, 128]], ["encoder.layers.2.feed_forward.layer_norm.weight", [256]], ["encoder.layers.2.feed_forward.layer_norm.bias", [256]], ["encoder.layers.2.feed_forward.dense.dense.weight", [512, 256]], ["encoder.layers.2.feed_forward.dense.dense.bias", [512]], ["encoder.layers.2.feed_forward.output.dense.weight", [256, 512]], ["encoder.layers.2.feed_forward.output.dense.bias", [256]], ["encoder.layers.3.attention.layer_norm.weight", [256]], ["encoder.layers.3.attention.layer_norm.bias", [256]], ["encoder.layers.3.attention.self_attention.query_key.weight", [128, 256]], ["encoder.layers.3.attention.self_attention.value.weight", [128, 256]], ["encoder.layers.3.attention.output.dense.weight", [256, 128]], ["encoder.layers.3.feed_forward.layer_norm.weight", [256]], ["encoder.layers.3.feed_forward.layer_norm.bias", [256]], ["encoder.layers.3.feed_forward.dense.dense.weight", [512, 256]], ["encoder.layers.3.feed_forward.dense.dense.bias", [512]], ["encoder.layers.3.feed_forward.output.dense.weight", [256, 512]], ["encoder.layers.3.feed_forward.output.dense.bias", [256]], ["encoder.layers.4.attention.layer_norm.weight", [256]], ["encoder.layers.4.attention.layer_norm.bias", [256]], ["encoder.layers.4.attention.self_attention.query.weight", [128, 256]], ["encoder.layers.4.attention.self_attention.key.weight", [128, 256]], ["encoder.layers.4.attention.self_attention.value.weight", [128, 256]], ["encoder.layers.4.attention.output.dense.weight", [256, 128]], ["encoder.layers.4.feed_forward.layer_norm.weight", [256]], ["encoder.layers.4.feed_forward.layer_norm.bias", [256]], ["encoder.layers.4.feed_forward.dense.dense.weight", [512, 256]], ["encoder.layers.4.feed_forward.dense.dense.bias", [512]], ["encoder.layers.4.feed_forward.output.dense.weight", [256, 512]], ["encoder.layers.4.feed_forward.output.dense.bias", [256]], ["encoder.layers.5.attention.layer_norm.weight", [256]], ["encoder.layers.5.attention.layer_norm.bias", [256]], ["encoder.layers.5.attention.self_attention.query_key.weight", [128, 256]], ["encoder.layers.5.attention.self_attention.value.weight", [128, 256]], ["encoder.layers.5.attention.output.dense.weight", [256, 128]], ["encoder.layers.5.feed_forward.layer_norm.weight", [256]], ["encoder.layers.5.feed_forward.layer_norm.bias", [256]], ["encoder.layers.5.feed_forward.dense.dense.weight", [512, 256]], ["encoder.layers.5.feed_forward.dense.dense.bias", [512]], ["encoder.layers.5.feed_forward.output.dense.weight", [256, 512]], ["encoder.layers.5.feed_forward.output.dense.bias", [256]], ["encoder.layer_norm.weight", [512]], ["encoder.layer_norm.bias", [512]]], "output_shape": [[8, 4608, 512], [[8, 4608, 256], [8, 4608, 256], [8, 4608, 256], [8, 4608, 256], [8, 4608, 256], [8, 4608, 256], [8, 4608, 256]]], "num_parameters": [7680, 4096, 13824, 256, 256, 32768, 32768, 32768, 32768, 256, 256, 131072, 512, 131072, 256, 256, 256, 32768, 32768, 32768, 256, 256, 131072, 512, 131072, 256, 256, 256, 32768, 32768, 32768, 32768, 256, 256, 131072, 512, 131072, 256, 256, 256, 32768, 32768, 32768, 256, 256, 131072, 512, 131072, 256, 256, 256, 32768, 32768, 32768, 32768, 256, 256, 131072, 512, 131072, 256, 256, 256, 32768, 32768, 32768, 256, 256, 131072, 512, 131072, 256, 512, 512]}, {"name": "lm_head", "id": 1622111870472, "class_name": "ReformerOnlyLMHead(\n  (decoder): Linear(in_features=512, out_features=30, bias=True)\n)", "parameters": [["bias", [30]], ["decoder.weight", [30, 512]]], "output_shape": [[8, 4608, 30]], "num_parameters": [30, 15360]}], "edges": []}